{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hadoop_spark_ventas.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG8nP7Wmz9Mr"
      },
      "source": [
        "### Universidad Nacional de Lujan - Bases de Datos Masivas (11088) - Cavasin Nicolas #143501\n",
        "\n",
        "# TP06: Frameworks de procesamiento distribuido\n",
        "\n",
        "## Hadoop MapReduce:\n",
        "El archivo *ventas.txt* posee las ventas de una empresa con los siguientes datos: *id_vendedor*, *id_coordinador*, *cantidad_de_productos_vendidos*, *cantidad_de_dinero*.\n",
        "\n",
        "Genere un esquema bajo el paradigma *MapReduce* para resolver las siguientes consignas:\n",
        "\n",
        "- Produzca un mapper y un reducer para responder cuál es el bonus obtenido por cada vendedor siendo que cada vendedor obtiene el 3% del total del dinero vendido.\n",
        "\n",
        "- Produzca un mapper y un reducer para obtener la cantidad de productos vendidos por cada vendedor, agrupado por coordinador.\n",
        "\n",
        "**Nota:** para facilitar la lectura se ha comentado la impresion por consola. Modificar a gusto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEEcA_TTzR8o",
        "outputId": "8a0e3b1b-ce63-4789-c0ba-c9e119bd81b0"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/bdm-unlu/2020/master/TPs/TP06/data/ventas.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-05 00:08:29--  https://raw.githubusercontent.com/bdm-unlu/2020/master/TPs/TP06/data/ventas.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3705191 (3.5M) [text/plain]\n",
            "Saving to: ‘ventas.txt’\n",
            "\n",
            "ventas.txt          100%[===================>]   3.53M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-12-05 00:08:30 (28.0 MB/s) - ‘ventas.txt’ saved [3705191/3705191]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdCV511dMuwc"
      },
      "source": [
        "Para emular el funcionamiento de Hadoop, se utilizarán 3 scripts:\n",
        "\n",
        "1. ``mapper.py``: \n",
        "    - Realiza la transformación al formato deseado ``<key, value>`` quedando ``<id_vendedor, cantidad_de_dinero>``.\n",
        "    - Escribe el resultado a un archivo *01-mapped_a.txt* en formato *.tsv* para que lo consuma el siguiente script.\n",
        "\n",
        "2. ``sorter.py``: \n",
        "    - Ordena la salida del ``script_mapper`` por *id_vendedor*.\n",
        "    - Escribe un nuevo archivo *02-sorted_b.txt* en formato *.tsv*.\n",
        "\n",
        "3. ``reducer.py``:\n",
        "    - Consume la salida del ``script_sorter``.\n",
        "    - Acumula y calcula el 3% del total de dinero que le corresponde a cada *id_vendedor*.\n",
        "    - Escribe los resultados en nuevo archivo *03-reduced_b.txt* en formato *.tsv*.\n",
        "\n",
        "___\n",
        "\n",
        "# Paso 1 - Mapping:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0H57-9u2MmT"
      },
      "source": [
        "# Abro el archivo ventas\n",
        "with open(\"ventas.txt\") as file_hdfs:\n",
        "\n",
        "    # Creo el archivo mapper.txt\n",
        "    with open('01-mapped_a.txt', 'w') as map_file:\n",
        "\n",
        "        # Leo cada linea del archivo\n",
        "        for line in file_hdfs:\n",
        "\n",
        "            # Se eliminan los espacios en blanco iniciales y finales\n",
        "            line = line.strip()\n",
        "\n",
        "            # Separo la linea en palabras y obtengo una lista \n",
        "            words = line.split()\n",
        "\n",
        "            # Imprimo el id_vendedor y cantidad_de_dinero\n",
        "            map_file.write(f'{words[0]}\\t{words[3]}\\n')\n",
        "\n",
        "# 'with' cierra automaticamente todos los archivos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB67aMhOQts4"
      },
      "source": [
        "# Paso 2 - Sorting:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_248mfJTFSo"
      },
      "source": [
        "# Abro el archivo mapped\n",
        "with open('01-mapped_a.txt', 'r') as map_file:\n",
        "\n",
        "    # Creo el archivo sorted\n",
        "    with open('02-sorted_a.txt', 'w') as sort_file:\n",
        "\n",
        "        # Ordeno y escribo el nuevo archivo ordenado\n",
        "        for line in sorted(map_file):\n",
        "            sort_file.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PELk6MFTF4I"
      },
      "source": [
        "# Paso 3 - Reducing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIC99W5rTEbE"
      },
      "source": [
        "# Referencias\n",
        "vendedor_actual = None\n",
        "dinero_actual = 0\n",
        "\n",
        "# Abro el archivo ordenado por id_vendedor\n",
        "with open('02-sorted_a.txt', 'r') as sort_file:\n",
        "\n",
        "    # Creo el archivo reducer\n",
        "    with open('03-reduced_a.txt', 'w') as red_file:\n",
        "\n",
        "        # Por cada linea del sorter\n",
        "        for line in sort_file:\n",
        "\n",
        "            # Obtengo el <key, value> de cada linea \n",
        "            vendedor, dinero = line.split('\\t')\n",
        "\n",
        "            # Convierto dinero a float\n",
        "            dinero = float(dinero)\n",
        "\n",
        "            # Si no hubo un cambio de vendedor acumulo\n",
        "            if vendedor == vendedor_actual:\n",
        "                dinero_actual += dinero\n",
        "            else:\n",
        "                # Si hubo un cambio de vendedor imprimo y actualizo\n",
        "\n",
        "                if vendedor:\n",
        "                    # Calculo el bono\n",
        "                    bono = dinero_actual * 0.03\n",
        "\n",
        "                    # Informo por consola - ELIMINAR COMENTARIO PARA VISUALIZAR\n",
        "                    #print(vendedor_actual, '\\t', bono)\n",
        "\n",
        "                    # Escribo tambien el archivo reduced\n",
        "                    red_file.write(f'{vendedor_actual}\\t{bono}\\n')\n",
        "\n",
        "                    # Modifico los 'actuales'\n",
        "                    vendedor_actual = vendedor\n",
        "                    dinero_actual = dinero\n",
        "\n",
        "\n",
        "# Agrego la ultima linea procesada\n",
        "bono = (dinero_actual + dinero)* 0.03\n",
        "\n",
        "# ELIMINAR COMENTARIO PARA VISUALIZAR\n",
        "# print(vendedor_actual, '\\t', bono)\n",
        "\n",
        "with open('03-reduced_a.txt', 'a') as red_file:\n",
        "    red_file.write(f'{vendedor_actual}\\t{bono}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n8GuO6Kd6JJ"
      },
      "source": [
        "___\n",
        "\n",
        "A continuación se realiza el mismo proceso pero para el punto b:\n",
        "\n",
        "1. ``mapper.py``: \n",
        "    - Realiza la transformación al formato deseado ``<key, value>`` quedando ``<[id_coordinador, id_vendedor], cantidad_de_productos_vendidos>``.\n",
        "    - Escribe el resultado a un archivo *01-mapped_b.txt* en formato *.tsv* para que lo consuma el siguiente script.\n",
        "\n",
        "2. ``sorter.py``: \n",
        "    - Ordena la salida del ``script_mapper`` por *id_coordinador* + *id_vendedor*.\n",
        "    - Escribe un nuevo archivo *02-sorted_b.txt* en formato *.tsv*.\n",
        "\n",
        "3. ``reducer.py``:\n",
        "    - Consume la salida del ``script_sorter``.\n",
        "    - Calcula el total de productos vendidos agrupados por coordinador y vendedor.\n",
        "    - Escribe los resultados en nuevo archivo *03-reduced_b.txt* en formato *.tsv*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSGpD2Z3egul"
      },
      "source": [
        "# Abro el archivo ventas\n",
        "with open(\"ventas.txt\") as file_hdfs:\n",
        "\n",
        "    # Creo el archivo mapper.txt\n",
        "    with open('01-mapped_b.txt', 'w') as map_file:\n",
        "\n",
        "        # Leo cada linea del archivo\n",
        "        for line in file_hdfs:\n",
        "\n",
        "            # Se eliminan los espacios en blanco iniciales y finales\n",
        "            line = line.strip()\n",
        "\n",
        "            # Separo la linea en palabras y obtengo una lista \n",
        "            words = line.split()\n",
        "\n",
        "            # Imprimo el id_coordinador, id_vendedor y cantidad_de_productos_vendidos\n",
        "            map_file.write(f'{words[1]}\\t{words[0]}\\t{words[2]}\\n')\n",
        "\n",
        "# 'with' cierra automaticamente todos los archivos\n",
        "\n",
        "# Abro el archivo mapped\n",
        "with open('01-mapped_b.txt', 'r') as map_file:\n",
        "\n",
        "    # Creo el archivo sorted\n",
        "    with open('02-sorted_b.txt', 'w') as sorted_file:\n",
        "\n",
        "        # Ordeno y escribo el nuevo archivo ordenado\n",
        "        for line in sorted(map_file):\n",
        "            sorted_file.write(line)\n",
        "\n",
        "# Referencias\n",
        "coordinador_actual = None\n",
        "vendedor_actual = None\n",
        "cantidad_actual = 0\n",
        "\n",
        "# Abro el archivo ordenado por id_vendedor\n",
        "with open('02-sorted_b.txt', 'r') as sorted_file:\n",
        "\n",
        "    # Creo el archivo reducer\n",
        "    with open('03-reduced_b.txt', 'w') as red_file:\n",
        "\n",
        "        # Por cada linea del sorted\n",
        "        for line in sorted_file:\n",
        "\n",
        "            # Obtengo el <key, value> de cada linea \n",
        "            coordinador, vendedor, cantidad = line.split('\\t')\n",
        "\n",
        "            # Convierto de string a integer\n",
        "            cantidad = int(dinero)\n",
        "\n",
        "            # Si no hubo un cambio de clave, acumulo\n",
        "            if coordinador == coordinador_actual and vendedor == vendedor_actual:\n",
        "                cantidad_actual += cantidad\n",
        "            else:\n",
        "                # Si hubo un cambio de coordinador/vendedor imprimo y actualizo\n",
        "\n",
        "                # Informo por consola - ELIMINAR COMENTARIO PARA VISUALIZAR\n",
        "                # print(coordinador_actual, '\\t', vendedor_actual, '\\t', cantidad_actual)\n",
        "\n",
        "                # Escribo tambien el archivo reduced\n",
        "                red_file.write(f'{coordinador_actual}\\t{vendedor_actual}\\t{cantidad_actual}\\n')\n",
        "\n",
        "                # Modifico los 'actuales'\n",
        "                coordinador_actual = coordinador\n",
        "                vendedor_actual = vendedor\n",
        "                cantidad_actual = cantidad\n",
        "\n",
        "# Agrego la ultima linea procesada - ELIMINAR COMENTARIO PARA VISUALIZAR\n",
        "# print(coordinador_actual, '\\t', vendedor_actual, '\\t', cantidad_actual+cantidad)\n",
        "\n",
        "with open('03-reduced_b.txt', 'a') as red_file:\n",
        "    red_file.write(f'{coordinador_actual}\\t{vendedor_actual}\\t{cantidad_actual+cantidad}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ4728Rtv-pC"
      },
      "source": [
        "## Apache Spark con PySpark:\n",
        "Resuelva el ejercicio anterior con PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BLdDdr6zNK_",
        "outputId": "df6a8c6f-6fac-4019-d9fc-89461a08d99f"
      },
      "source": [
        "# Instalo pyspark y configuro el entorno\n",
        "!pip install pyspark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 71kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=5e7e9e88581937b42dff3b719f29474490cbbc7ffc3ba1e5aeecbe5b9e0ea223\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "L610N4Cu0aUC",
        "outputId": "7801fb55-885b-4680-bb5e-3e5e10a94f4c"
      },
      "source": [
        "# Importo Spark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# Seteo el master al entorno local y defino el nombre de la app para identificarla\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"Bono vendedores\")\n",
        "\n",
        "# Inicializo el Spark Context\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "# Verifico inicializacion\n",
        "sc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://dfbf25aee644:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Bono vendedores</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ],
            "text/plain": [
              "<SparkContext master=local appName=Bono vendedores>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgmv7KtK0aNS",
        "outputId": "c33f9725-b689-4abf-cc3d-23c060eec725"
      },
      "source": [
        "# Leo el archivo de ventas y separo cada valor \n",
        "rdd_ventas = sc.textFile(\"ventas.txt\").map(lambda line: line.split(\"\\t\"))\n",
        "\n",
        "# Muestro la primer linea del archivo\n",
        "print(f'Primer linea del archivo: {rdd_ventas.first()}')\n",
        "print(f'Ocurrencias en archivo: {rdd_ventas.count()}.\\n')\n",
        "\n",
        "# Mapeo seleccionando id_vendedor y cantidad_de_dinero (columnas 0 y 3)\n",
        "# Ademas convierto dinero a float y ordeno por id_vendedor\n",
        "rdd_vendedor_dinero = rdd_ventas.map(lambda values: (int(values[0]), float(values[3])))\n",
        "\n",
        "# Muestro el primer mapeo\n",
        "print(f'Primer elemento del mapper: <{rdd_vendedor_dinero.first()}>')\n",
        "print(f'Ocurrencias en mapper: {rdd_vendedor_dinero.count()}.\\n')\n",
        "\n",
        "# Ahora reduzco por id_vendedor y acumulo el dinero\n",
        "rdd_vendedor_dinero = rdd_vendedor_dinero.reduceByKey(lambda id, dinero: id +dinero)\n",
        "\n",
        "# Mapeo nuevamente para poder aplicar una multiplicacion y asi\n",
        "# calcular el bono de cada vendedor\n",
        "rdd_vendedor_bono = rdd_vendedor_dinero.map(lambda values: (values[0], values[1]*0.03))\n",
        "\n",
        "# Muestro primer reduccion y ocurrencias\n",
        "print(f'Primer elemento del reducer: <{rdd_vendedor_bono.first()}>')\n",
        "print(f'Ocurrencias en reducer: {rdd_vendedor_bono.count()}.')\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Primer linea del archivo: ['17493', '6012', '21', '207.509827822219']\n",
            "Ocurrencias en archivo: 119582.\n",
            "\n",
            "Primer elemento del mapper: <(17493, 207.509827822219)>\n",
            "Ocurrencias en mapper: 119582.\n",
            "\n",
            "Primer elemento del reducer: <(100, 286.54551868222)>\n",
            "Ocurrencias en reducer: 15522.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VJzWYt3cVQf"
      },
      "source": [
        "Por último se muestra todo el contenido del RDD. Es decir, todos los pares ``<id_vendedor, bono>``:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYm5IghuXo7K"
      },
      "source": [
        "# Muestro todos los id_vendedor con su correspondiente bono\n",
        "# Tanto el sortByKey() como el collect() son instrucciones costosas en \n",
        "# terminos de procesamiento\n",
        "for vendedor in rdd_vendedor_bono.sortByKey().collect():\n",
        "    print(vendedor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0SziedOPFCo"
      },
      "source": [
        "# Finalizo la aplicacion\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dkmpFoJcmRH"
      },
      "source": [
        "\n",
        "___\n",
        "\n",
        "\n",
        "A continuación se realiza el punto b:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iCYHKCGcl3O",
        "outputId": "9031d103-4289-4f4a-b947-a131e995e80b"
      },
      "source": [
        "# Nuevamente seteo el master al entorno local y defino el nombre de la app para identificarla\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"Cantidad vendida por coordinador_vendedor\")\n",
        "\n",
        "# Inicializo el Spark Context\n",
        "sc = SparkContext(conf = conf)\n",
        "\n",
        "# Leo el archivo\n",
        "rdd_ventas = sc.textFile(\"ventas.txt\").map(lambda line: line.split(\"\\t\"))\n",
        "\n",
        "# Reutilizo el rdd del punto anterior \n",
        "# Mapeo <[id_coordinador, id_vendedor], cantidad_productos_vendidos> \n",
        "# Ademas, convierto cantidades a integer\n",
        "rdd_coord_ventas = rdd_ventas.map(lambda values: ((int(values[1]), int(values[0])), int(values[2])))\n",
        "\n",
        "# Muestro el primer mapeo\n",
        "print(f'Primer elemento del mapper: <{rdd_coord_ventas.first()}>')\n",
        "print(f'Ocurrencias en mapper: {rdd_coord_ventas.count()}.')\n",
        "print()\n",
        "\n",
        "# Acumulo las cantidades por clave\n",
        "rdd_coord_cantidades = rdd_coord_ventas.reduceByKey(lambda coord, cant: coord +cant)\n",
        "\n",
        "# Muestro la primer reduccion\n",
        "print(f'Primer elemento del reducer: <{rdd_coord_cantidades.first()}>')\n",
        "print(f'Ocurrencias en reducer: {rdd_coord_cantidades.count()}.')\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Primer elemento del mapper: <((6012, 17493), 21)>\n",
            "Ocurrencias en mapper: 119582.\n",
            "\n",
            "Primer elemento del reducer: <((6012, 17493), 236)>\n",
            "Ocurrencias en reducer: 15522.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZMxeEBgocf7"
      },
      "source": [
        "# Muestro todo el contenido del rdd reducido\n",
        "# Tanto el sortByKey() como el collect() son instrucciones costosas en \n",
        "# terminos de procesamiento\n",
        "for val in rdd_coord_cantidades.sortByKey().collect():\n",
        "    print(val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeOaN4I4ob0D"
      },
      "source": [
        "# Finalizo la aplicacion\n",
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}