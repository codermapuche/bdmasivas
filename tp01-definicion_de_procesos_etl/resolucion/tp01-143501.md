# Trabajo práctico 1 - Definición de procesos ETL

## 1) Migración a Postgres sin Pentaho
Se cuenta con el dataset *Medios* que cuenta con 7000 medios nacionales. Se desea normalizar esta información en una Base de Datos transaccional teniendo en cuenta que cada medio posee atributos correspondientes a su nombre, ubicación, tipo de medio y especialidad. Migre la información del archivo a una Base de Datos PostgreSQL con la siguiente estructura:

- Medios(id, nombre, id_especialidad, id_tipo_medio, dirección, id_ciudad).
- Especialidades(id, descripción).
- Tipos_medio(id, descripción).
- Ciudades(id, nombre, id_provincia).
- Provincias(id, nombre).

Explique  someramente  la  metodología  utilizada  y  estime  el  tiempo  que  le demandó la actividad.

___

***Resolución:*** 


1. Se descargó la librería [pandas][0] tal como lo recomienda [este][1] sitio para poder leer datos en formato ``.xls`` desde un script de python. Adicionalmente se instaló también mediante [pip][2] la librería ``xlrd`` para obtener retro-compatibilidad con viejos archivos de excel que hagan uso de la extensión ``.xls``.

[0]: https://pandas.pydata.org/
[1]: https://datatofish.com/read_excel/
[2]: https://pypi.org/project/pip/

2. Obtenido el acceso a los datos, comenzó el filtrado de los mismos para poder construir las tablas requeridas. Los datos que ``pandas`` reconoce como ``NaN`` (por ser celdas de excel vacías) fueron reemplazados por ``""`` (string vacío) para facilitar su manipulación en la DB. El resto, se leyó por columnas y se crearon ids por cada valor diferente encontrado en dicha columna. Se almacenó cada futura tabla en diccionarios, siendo ``key``= valor de columna y ``value``=id unívoco.

3. Organizados los datos, se procedió a establecer la conexión con la base de datos. Para ello, se instaló la librería [psycopg2][3] que permite establecer conexiones con DBs Postgres. Posteriormente, se levantó el docker proporcionado [aquí][4] que ya contiene la DB. Finalmente, se creó la conexión entre el script y la DB alojada en dicho docker.

[3]: https://pypi.org/project/psycopg2/
[4]: https://github.com/bdm-unlu/2020/tree/master/dockers

4. Establecida la conexión y organizados los datos, solo restó la creación de tablas y la inserción de los datos organizados haciendo uso de los diccionarios creados en el paso 2.

5. Es importante destacar lo siguiente: la creación del script tomó unos minutos. Sin embargo la lectura de las documentaciones mencionadas, en total, demoró ~2hs. 



## 2) ETL con Pentaho:
Se cuenta con los orígenes de datos *etl_cursadas*, *etl_estudiantes* y *planes* con información de los estudiantes de la Universidad y sus cursadas durante el 1er Cuatrimestre 2003. Se solicita que genere una nueva DB con la siguiente estructura:

- rendimiento_academico(id_estudiante, id_plan, id_sede, id_ciudad, id_sexo, id_cohorte, cantidad_cursadas, cantidad_aprobadas, promedio).
- planes(id_plan, codigo_plan, codigo_carrera, nombre_carrera).
- ciudades(id_ciudad, codigo_postal, nombre_ciudad, provincia).
- sedes(id_sede, sede).
- sexo(id_sexo, sexo).
- cohortes(id_cohortes, cohorte).

Utilice el software PDI y estime el tiempo que le demandó la actividad.

___

***Mapeo:***

A continuación se explica el mapeo realizado entre cada archivo y las tablas de la resultante DB:

- Tabla cohortes(id_cohortes, cohorte):
	- id_cohorte -> serie autoincremental generada por Postgres.
	- cohortes -> columna cohorte(etl_estudiantes.csv).


- Tabla sexo(id_sexo, sexo):
	- id_sexo -> serie autoincremental generada por Postgres.
	- sexo -> columna sexo (etl_estudiantes.csv).


- Tabla sedes(id_sede, sede):
	- id_sede -> serie autoincremental generada por Postgres.
	- sede -> columna sede (etl_estudiantes.csv).


- Tabla ciudades(id_ciudad, codigo_postal, nombre_ciudad, provincia):
	- id_ciudad -> serie autoincremental generada por Postgres.
	- codigo_postal -> columna codigo_postal (etl_estudiantes.csv).
	- nombre_ciudad -> columna localidad (etl_estudiantes.csv).
	- provincia -> columna provincia (etl_estudiantes.csv).
	

- Tabla planes(id_plan, codigo_plan, codigo_carrera, nombre_carrera):
	- id_plan -> serie autoincremental generada por Postgres.
	- codigo_plan -> columna codigo_plan (texto_parseado.csv).
	- codigo_carrera -> ? *quedará nulo porque no sé de donde sale ese dato*.
	- nombre_carrera -> columna nombre_carrera (texto_parseado.csv).


- Tabla rendimiento_académico(id_estudiante, id_plan, id_sede, id_ciudad, id_sexo, id_cohorte, cantidad_cursadas, cantidad_aprobadas, promedio):
	- id_estudiante -> columna id_estudiante (etl_estudiantes.csv) + columna id_estudiante (etl_cursadas.sql).
	- id plan -> mapeo entre id_plan (tabla planes) y columna id_plan (tabla planes).
	- id_sede -> mapeo entre id_sede (tabla sedes) y columna sede (etl_estudiantes.csv).
	- id_ciudad -> mapeo entre id_ciudad (tabla ciudades) y columnas codigo_postal, localidad (etl_estudiantes.csv).
	- id_sexo -> mapeo entre id_sexo (tabla sexo) y columna sexo (etl_estudiantes.csv).
	- id_cohorte -> mapeo entre id_cohorte (tabla cohortes) y columna cohorte (etl_estudiantes.csv).
	- cantidad_cursadas -> suma de diferentes valores de la columna asignatura (etl_cursadas.sql).
	- cantidad_aprobadas -> suma de valores > 4 de la columna calificación (etl_cursadas.sql).
	- promedio -> avg de columna calificación (etl_cursadas.sql).
	- Sin PK, porque el id_estudiante no es una ``SERIE`` propia de la DB.

***Resolución:***

Establecido el mapeo entre entre los archivos y las tablas, comenzó el proceso ETL:

1. Se dividió el archivo *01-02-etl_cursadas.sql* en dos:
   1. Se extrajo el dump relacionado a la creación de la tabla y se lo guardó como *creacion_tablas_1.sql* para ser utilizado más adelante como script en el job. 
   2. Se extrajo el dump con la copia de las tuplas y se lo guardó como *etl_cursadas_filtrado.txt* en el directorio *00_ds_filtrado/*.

2. El archivo *etl_cursadas_filtrado.txt* se subió al postgres para ser utilizado más adelante durante la transformación.

3. Se modificó el archivo *01-02-planes.txt* dos veces para facilitar su procesamiento en Pentaho-DI:
	1. Con un editor de texto para eliminar los headers y footers.
	2. Con el script *01_filtro_planes.py* para eliminar los espacios y convertirlo en un archivo ``.csv`` delimitado por ``;``.
	3. Se lo guardó como *etl_planes_filtrado.csv* en el directorio *00_ds_filtrado/*.


4. Se creó el job *08-job.kjb* para ejecutar los scripts de creacion de tablas mencionados. 

5. Se realiza la importación de datos mediante el archivo *etl_cursadas_filtrado.txt* sobre la tabla cursadas_2003_etl.

6. Se creó *02_transfo_planes.ktr* para procesar *00_ds_filtrado/etl_planes_filtrado.csv* y volcarlo a la DB respetando el mapeo mencionado arriba.

7. Se crearon las siguientes transformaciones para procesar el archivo *etl_estudiantes.csv* (que fué el único que no sufrió modificaciones de formato) respetando el mapeo mencionado arriba:
   1. *03_transfo_cohortes.ktr* para poblar la tabla cohortes.
   2. *04_transfo_sexos.ktr* para poblar la tabla sexo.
   3. *05_transfo_sedes.ktr* para poblar la tabla sedes.
   4. *06_transfo_ciudades.ktr* para poblar la tabla ciudades.

8. Se creó la transformación *07_transfo_rendimiento_academico.ktr* para crear la tabla rendimiento academico respetando el mapeo mencionado más arriba y haciendo uso de los datos importados a las tablas anteriormente creadas y pobladas. No se aplica restricción de clave primaria sobre la columna ``id_estudiante`` pues no es una *serie* propia de la DB y por lo tanto puede haber valores repetidos que deberán ser filtrados durante el preprocesamiento.


9.  Se creó la transformación *08-transfo_promedio.ktr* para manipular los datos de la transformación anterior y obtener el promedio de cada estudiante.

10. Por último, se modificó el job *09-job.kjb* y se agregó la ejecución de las transformaciones sobre las tablas y la ejecución del script encargado de eliminar la tabla *cursadas_2003_etl* que ya no será utilizada.
   
La realización de un ETL con Pentaho es más simple que cuando se involucra código. Sin embargo, al ser una herramienta nueva, se demoró bastante tiempo -un día casi entero- para hacer las instalaciones correctamente y adaptarse a su funcionamiento. Dicho esto, es fácil percibir que con un poco de práctica se pueden hacer cosas importantes de manera simple, rápida e integrada.


## 3). Rehacer 1) utilizando Pentaho:
Ahora, resuelva la consigna 1) con la herramienta PDI de la suite Pentaho, a través de las transformations y jobs necesarias para llevar adelante la solución. Tome el tiempo que demora en resolver este ejercicio con PDI.

___

***Resolución:***

1. Se crea el script *00-creacion_tablas.sql* respetando las estructuras planteadas en el ejercicio 1) y no se aplica la restricción de clave primaria sobre la columna ``id_medio`` ya que puede haber valores repetidos que deberán ser filtrados durante el preprocesamiento.

2. Se crean las siguientes transformaciones para procesar el archivo *01-01-Medios.csv*:
   1. *01-transfo_provincias.ktr* para poblar la tabla provincias.
   2. *02-transfo_tipos_medio.ktr* para poblar la tabla tipos_medio.
   3. *03-transfo_ciudades.ktr* para poblar la tabla ciudades.
   4. *04-transfo_especialidades* para poblar la tabla especialidades.
   5. *05-transfo_medios* para poblar la tabla medios.
   
3. Se crea el *06-job.kjb* para controlar el script de creación de tablas y la ejecución correcta de las transformaciones.

La re-realización de este punto utilizando Pentaho demoró un tiempo similar al que se demoró haciéndolo con python, pues se procesó un único archivo. Cuando las fuentes son más, como en el punto 2, está claro que Pentaho es superior en cuanto eficiencia de tiempo empleado para resolver la tarea así también como el nivel de abstracción manejado.


## 4) Job de actualización:
Cree un Job que verifique todos los días a las 14 hs si existe el archivo *01-01-Medios.csv*, trabajado en el punto 1), en un  directorio determinado y en caso afirmativo ejecute el Job para actualizar la DB generada antes.

___

***Resolución:***

1. El archivo correspondiente se llama *punto_4.kjb*.
