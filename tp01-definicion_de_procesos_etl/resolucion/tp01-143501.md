# Trabajo práctico 1 - Definición de procesos ETL

### 1) Migración a Postgres sin Pentaho
Se cuenta con el dataset *Medios* que cuenta con 7000 medios nacionales. Se desea normalizar esta información en una Base de Datos transaccional teniendo en cuenta que cada medio posee atributos correspondientes a su nombre, ubicación, tipo de medio y especialidad. Migre la información del archivo a una Base de Datos PostgreSQL con la siguiente estructura:

- Medios(id, nombre, id_especialidad, id_tipo_medio, dirección, id_ciudad).
- Especialidades(id, descripción).
- Tipos_medio(id, descripción).
- Ciudades(id, nombre, id_provincia).
- Provincias(id, nombre).

Explique  someramente  la  metodología  utilizada  y  estime  el  tiempo  que  le demandó la actividad.

___

***Resolución:*** 


1. Se descargó la librería [pandas][0] tal como lo recomienda [este][1] sitio para poder leer datos en formato ``.xls`` desde un script de python. Adicionalmente se instaló también mediante [pip][2] la librería ``xlrd`` para obtener retro-compatibilidad con viejos archivos de excel que hagan uso de la extensión ``.xls``.

[0]: https://pandas.pydata.org/
[1]: https://datatofish.com/read_excel/
[2]: https://pypi.org/project/pip/

2. Obtenido el acceso a los datos, comenzó el filtrado de los mismos para poder construir las tablas requeridas. Los datos que ``pandas`` reconoce como ``NaN`` (por ser celdas de excel vacías) fueron reemplazados por ``""`` (string vacío) para facilitar su manipulación en la DB. El resto, se leyó por columnas y se crearon ids por cada valor diferente encontrado en dicha columna. Se almacenó cada futura tabla en diccionarios, siendo ``key``= valor de columna y ``value``=id unívoco.

3. Organizados los datos, se procedió a establecer la conexión con la base de datos. Para ello, se instaló la librería [psycopg2][3] que permite establecer conexiones con DBs Postgres. Posteriormente, se levantó el docker proporcionado [aquí][4] que ya contiene la DB. Finalmente, se creó la conexión entre el script y la DB alojada en dicho docker.

[3]: https://pypi.org/project/psycopg2/
[4]: https://github.com/bdm-unlu/2020/tree/master/dockers

4. Establecida la conexión y organizados los datos, solo restó la creación de tablas y la inserción de los datos organizados haciendo uso de los diccionarios creados en el paso 2.

5. Es importante destacar lo siguiente: la creación del script tomó unos minutos. Sin embargo la lectura de las documentaciones mencionadas, en total, demoró ~2hs. 



### 2) ETL con Pentaho:
Se cuenta con los orígenes de datos *etl_cursadas*, *etl_estudiantes* y *planes* con información de los estudiantes de la Universidad y sus cursadas durante el 1er Cuatrimestre 2003. Se solicita que genere una nueva DB con la siguiente estructura:

1. rendimiento_academico(id_estudiante, id_plan, id_sede, id_ciudad, id_sexo, id_cohorte, cantidad_cursadas, cantidad_aprobadas, promedio).
2. planes(id_plan, codigo_plan, codigo_carrera, nombre_carrera).
3. ciudades(id_ciudad, codigo_postal, nombre_ciudad, provincia).
4. sedes(id_sede, sede).
5. sexo(id_sexo, sexo).
6. cohortes(id_cohortes, cohorte).

Utilice el software PDI y estime el tiempo que le demandó la actividad.

___


***Anotaciones:***

1. El archivo *01-02-planes.txt* fué modificado dos veces para facilitar su procesamiento en Pentaho DI:
	- Con un editor de texto para eliminar los headers y footers.
	- Con el script *filtrar_planes.py* para eliminar los espacios y convertirlo en un archivo ``.csv`` delimitado por ``;``.

2. El archivo *01-02-etl_cursadas.sql* fué modificado con un editor de texto para poder tratarlo como un archivo ``.csv`` debido a que el driver JDBC [no soporta la ejecución de instrucciones ``COPY``][5] provenientes de algún *dump*. 

	- Aprovechando que en el archivo las columnas de la tabla se encontraban separadas por ``TAB``s horizontales, eliminé los demás datos y lo guardé como un ``.csv`` delimitado por ``TAB``.

[5]: https://github.com/pgjdbc/pgjdbc/issues/1299


___


***Resolución:***

A continuación se explica el mapeo realizado entre cada archivo y las tablas de la resultante DB:

1. Tabla cohortes(id_cohortes, cohorte):
	- id_cohorte -> serie autoincremental generada por Postgres.
	- cohortes -> columna cohorte(etl_estudiantes.csv).


2. Tabla sexo(id_sexo, sexo):
	- id_sexo -> serie autoincremental generada por Postgres.
	- sexo -> columna sexo (etl_estudiantes.csv).


3. Tabla sedes(id_sede, sede):
	- id_sede -> serie autoincremental generada por Postgres.
	- sede -> columna sede (etl_estudiantes.csv).


4. Tabla ciudades(id_ciudad, codigo_postal, nombre_ciudad, provincia):
	- id_ciudad -> serie autoincremental generada por Postgres.
	- codigo_postal -> columna codigo_postal (etl_estudiantes.csv).
	- nombre_ciudad -> columna localidad (etl_estudiantes.csv).
	- provincia -> columna provincia (etl_estudiantes.csv).
	

5. Tabla planes(id_plan, codigo_plan, codigo_carrera, nombre_carrera):
	- id_plan -> serie autoincremental generada por Postgres.
	- codigo_plan -> columna codigo_plan (texto_parseado.csv).
	- codigo_carrera -> ? *quedará nulo porque no sé de donde sale ese dato*.
	- nombre_carrera -> columna nombre_carrera (texto_parseado.csv).


6. Tabla rendimiento_académico(id_estudiante, id_plan, id_sede, id_ciudad, id_sexo, id_cohorte, cantidad_cursadas, cantidad_aprobadas, promedio):
	- id_estudiante -> columna id_estudiante (etl_estudiantes.csv) + columna id_estudiante (etl_cursadas.sql).
	- id plan -> mapeo entre id_plan (tabla planes) y columna id_plan (tabla planes).
	- id_sede -> mapeo entre id_sede (tabla sedes) y columna sede (etl_estudiantes.csv).
	- id_ciudad -> mapeo entre id_ciudad (tabla ciudades) y columnas codigo_postal, localidad (etl_estudiantes.csv).
	- id_sexo -> mapeo entre id_sexo (tabla sexo) y columna sexo (etl_estudiantes.csv).
	- id_cohorte -> mapeo entre id_cohorte (tabla cohortes) y columna cohorte (etl_estudiantes.csv).
	- cantidad_cursadas -> suma de diferentes valores de la columna asignatura (etl_cursadas.sql).
	- cantidad_aprobadas -> suma de valores > 4 de la columna calificación (etl_cursadas.sql).
	- promedio -> avg de columna calificación (etl_cursadas.sql).
	- PK (id_estudiante).

7. La realización de un ETL con Pentaho es más simple que cuando se involucra código. Sin embargo, al ser una herramienta nueva, se demoró bastante tiempo -un día casi entero- entre hacer las instalaciones correctamente y adaptarse a su funcionamiento. Dicho esto, es fácil percibir que con un poco de práctica se pueden hacer cosas importantes de manera simple, rápida e integrada.


### 3). Rehacer 1) utilizando Pentaho:
Ahora, resuelva la consigna 1) con la herramienta PDI de la suite Pentaho, a través de las transformations y jobs necesarias para llevar adelante la solución. Tome el tiempo que demora en resolver este ejercicio con PDI.

___

***Resolución:***



































